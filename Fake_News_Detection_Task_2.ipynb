{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2zK2o9B4wPA",
        "outputId": "f044cc66-1a20-432a-bce3-8dacf50fbe7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji==1.7.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m174.1/175.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=10badd1422f013f1346640d7a9b92f60a6f24dbd27b8105b38d486e63597cc26\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=3bc75c65df508e5947c6386c8206196f3b11adb6b52f7a97c094a8392be0bb00\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji==1.7.0\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NfTTnIdLD_tM",
        "outputId": "3bf847f6-91af-4bae-9f03-e2d334cc79b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow-text)\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.69.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow-text)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.1.2)\n",
            "Downloading tensorflow_text-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-text-2.18.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              },
              "id": "025804b2503e4b1d9a2efb1d3511567e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    This function cleans the input text by removing special characters,\n",
        "    converting to lowercase, and removing extra spaces.\n",
        "    \"\"\"\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub('\\[.*?\\]', '', text) # Remove text in square brackets\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n",
        "    text = re.sub('<.*?>+', '', text) # Remove HTML tags\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
        "    text = re.sub('\\n', '', text) # Remove newline characters\n",
        "    text = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters except spaces\n",
        "    text = re.sub(' +', ' ', text) # Remove extra spaces\n",
        "\n",
        "    return text\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "!pip install tensorflow-text  # Install the tensorflow-text package\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Import Tokenizer from tensorflow.keras instead of keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # Import pad_sequences from tensorflow.keras instead of keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('fake_news_classification_mal_train.csv')\n",
        "\n",
        "# Preprocess and clean text using the defined clean_text function\n",
        "train_data['Cleaned_News'] = train_data['News'].apply(clean_text)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_data['Cleaned_News'])\n",
        "X = tokenizer.texts_to_sequences(train_data['Cleaned_News'])\n",
        "X = pad_sequences(X, padding='post', maxlen=100)\n",
        "\n",
        "# Labels\n",
        "y = pd.get_dummies(train_data['Label']).values\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=100, input_length=100))\n",
        "model.add(LSTM(units=128, return_sequences=False))\n",
        "model.add(Dense(5, activation='softmax'))  # 5 categories\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=5, batch_size=64)\n",
        "\n",
        "# Predict on test data (after tokenizing and padding)\n",
        "test_data = pd.read_csv('Fake_test_without_labels.csv')\n",
        "test_data['Cleaned_News'] = test_data['text'].apply(clean_text)\n",
        "X_test = tokenizer.texts_to_sequences(test_data['Cleaned_News'])\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=100)\n",
        "\n",
        "# Predict and save results\n",
        "y_pred = model.predict(X_test)\n",
        "predicted_labels = np.argmax(y_pred, axis=1)\n",
        "test_data['Predicted_Label'] = predicted_labels\n",
        "\n",
        "# Save predictions\n",
        "test_data[['Id', 'Predicted_Label']].to_csv('test_predictions.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1nCW5smEphe",
        "outputId": "00230cb3-bd3d-4dc9-8f66-63a85bb8dd37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.18.1)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.1.2)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.5619 - loss: 1.2868\n",
            "Epoch 2/5\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.6410 - loss: 1.1185\n",
            "Epoch 3/5\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 177ms/step - accuracy: 0.6241 - loss: 1.1386\n",
            "Epoch 4/5\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 177ms/step - accuracy: 0.6501 - loss: 1.0961\n",
            "Epoch 5/5\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 209ms/step - accuracy: 0.6419 - loss: 1.1109\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# True labels from the test set\n",
        "y_test_true = pd.get_dummies(test_data['Predicted_Label']).values  # Assuming 'Label' contains the ground truth\n",
        "\n",
        "# Predicted labels\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(np.argmax(y_test_true, axis=1), y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Classification report for detailed metrics\n",
        "print(classification_report(np.argmax(y_test_true, axis=1), y_pred_classes))\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('test_predictions.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxDvpQOvFIgZ",
        "outputId": "e819c9f8-6e58-4ec8-d2dc-9b5eb222d105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1019\n",
            "\n",
            "    accuracy                           1.00      1019\n",
            "   macro avg       1.00      1.00      1.00      1019\n",
            "weighted avg       1.00      1.00      1.00      1019\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('test_predictions.csv')\n",
        "# Save the DataFrame as a TSV file\n",
        "df.to_csv('test_predictions.tsv', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "rhFYMfJ4HCMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('test_predictions.tsv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fexghZQ5HrCX",
        "outputId": "ec73f75c-4c99-419c-b52d-59d80d239cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1d9afa76-47a5-42ca-b44f-0d12e6822484\", \"test_predictions.tsv\", 11149)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_lines_with_1(file_path):\n",
        "    \"\"\"Prints lines from a file that contain the digit '1'.\"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if '1' in line:  # Check if '1' is present in the line\n",
        "                print(line, end='')  # Print the line (end='' prevents extra newline)\n",
        "\n",
        "# Call the function with the file path\n",
        "print_lines_with_1('test_predictions.tsv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1txD2n6kIduv",
        "outputId": "f02d21fe-c80c-42ba-d332-71889fbaf837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake_01\t0\n",
            "Fake_10\t0\n",
            "Fake_11\t0\n",
            "Fake_12\t0\n",
            "Fake_13\t0\n",
            "Fake_14\t0\n",
            "Fake_15\t0\n",
            "Fake_16\t0\n",
            "Fake_17\t0\n",
            "Fake_18\t0\n",
            "Fake_19\t0\n",
            "Fake_21\t0\n",
            "Fake_31\t0\n",
            "Fake_41\t0\n",
            "Fake_51\t0\n",
            "Fake_61\t0\n",
            "Fake_71\t0\n",
            "Fake_81\t0\n",
            "Fake_91\t0\n",
            "Fake_100\t0\n",
            "Fake_101\t0\n",
            "Fake_102\t0\n",
            "Fake_103\t0\n",
            "Fake_104\t0\n",
            "Fake_105\t0\n",
            "Fake_106\t0\n",
            "Fake_107\t0\n",
            "Fake_108\t0\n",
            "Fake_109\t0\n",
            "Fake_110\t0\n",
            "Fake_111\t0\n",
            "Fake_112\t0\n",
            "Fake_113\t0\n",
            "Fake_114\t0\n",
            "Fake_115\t0\n",
            "Fake_116\t0\n",
            "Fake_117\t0\n",
            "Fake_118\t0\n",
            "Fake_119\t0\n",
            "Fake_120\t0\n",
            "Fake_121\t0\n",
            "Fake_122\t0\n",
            "Fake_123\t0\n",
            "Fake_124\t0\n",
            "Fake_125\t0\n",
            "Fake_126\t0\n",
            "Fake_127\t0\n",
            "Fake_128\t0\n",
            "Fake_129\t0\n",
            "Fake_130\t0\n",
            "Fake_131\t0\n",
            "Fake_132\t0\n",
            "Fake_133\t0\n",
            "Fake_134\t0\n",
            "Fake_135\t0\n",
            "Fake_136\t0\n",
            "Fake_137\t0\n",
            "Fake_138\t0\n",
            "Fake_139\t0\n",
            "Fake_140\t0\n",
            "Fake_141\t0\n",
            "Fake_142\t0\n",
            "Fake_143\t0\n",
            "Fake_144\t0\n",
            "Fake_145\t0\n",
            "Fake_146\t0\n",
            "Fake_147\t0\n",
            "Fake_148\t0\n",
            "Fake_149\t0\n",
            "Fake_150\t0\n",
            "Fake_151\t0\n",
            "Fake_152\t0\n",
            "Fake_153\t0\n",
            "Fake_154\t0\n",
            "Fake_155\t0\n",
            "Fake_156\t0\n",
            "Fake_157\t0\n",
            "Fake_158\t0\n",
            "Fake_159\t0\n",
            "Fake_160\t0\n",
            "Fake_161\t0\n",
            "Fake_162\t0\n",
            "Fake_163\t0\n",
            "Fake_164\t0\n",
            "Fake_165\t0\n",
            "Fake_166\t0\n",
            "Fake_167\t0\n",
            "Fake_168\t0\n",
            "Fake_169\t0\n",
            "Fake_170\t0\n",
            "Fake_171\t0\n",
            "Fake_172\t0\n",
            "Fake_173\t0\n",
            "Fake_174\t0\n",
            "Fake_175\t0\n",
            "Fake_176\t0\n",
            "Fake_177\t0\n",
            "Fake_178\t0\n",
            "Fake_179\t0\n",
            "Fake_180\t0\n",
            "Fake_181\t0\n",
            "Fake_182\t0\n",
            "Fake_183\t0\n",
            "Fake_184\t0\n",
            "Fake_185\t0\n",
            "Fake_186\t0\n",
            "Fake_187\t0\n",
            "Fake_188\t0\n",
            "Fake_189\t0\n",
            "Fake_190\t0\n",
            "Fake_191\t0\n",
            "Fake_192\t0\n",
            "Fake_193\t0\n",
            "Fake_194\t0\n",
            "Fake_195\t0\n",
            "Fake_196\t0\n",
            "Fake_197\t0\n",
            "Fake_198\t0\n",
            "Fake_199\t0\n",
            "Fake_201\t0\n",
            "Fake_210\t0\n",
            "Fake_211\t0\n",
            "Fake_212\t0\n",
            "Fake_213\t0\n",
            "Fake_214\t0\n",
            "Fake_215\t0\n",
            "Fake_216\t0\n",
            "Fake_217\t0\n",
            "Fake_218\t0\n",
            "Fake_219\t0\n",
            "Fake_221\t0\n",
            "Fake_231\t0\n",
            "Fake_241\t0\n",
            "Fake_251\t0\n",
            "Fake_261\t0\n",
            "Fake_271\t0\n",
            "Fake_281\t0\n",
            "Fake_291\t0\n",
            "Fake_301\t0\n",
            "Fake_310\t0\n",
            "Fake_311\t0\n",
            "Fake_312\t0\n",
            "Fake_313\t0\n",
            "Fake_314\t0\n",
            "Fake_315\t0\n",
            "Fake_316\t0\n",
            "Fake_317\t0\n",
            "Fake_318\t0\n",
            "Fake_319\t0\n",
            "Fake_321\t0\n",
            "Fake_331\t0\n",
            "Fake_341\t0\n",
            "Fake_351\t0\n",
            "Fake_361\t0\n",
            "Fake_371\t0\n",
            "Fake_381\t0\n",
            "Fake_391\t0\n",
            "Fake_401\t0\n",
            "Fake_410\t0\n",
            "Fake_411\t0\n",
            "Fake_412\t0\n",
            "Fake_413\t0\n",
            "Fake_414\t0\n",
            "Fake_415\t0\n",
            "Fake_416\t0\n",
            "Fake_417\t0\n",
            "Fake_418\t0\n",
            "Fake_419\t0\n",
            "Fake_421\t0\n",
            "Fake_431\t0\n",
            "Fake_441\t0\n",
            "Fake_451\t0\n",
            "Fake_461\t0\n",
            "Fake_471\t0\n",
            "Fake_481\t0\n",
            "Fake_491\t0\n",
            "Fake_501\t0\n",
            "Fake_510\t0\n",
            "Fake_511\t0\n",
            "Fake_512\t0\n",
            "Fake_513\t0\n",
            "Fake_514\t0\n",
            "Fake_515\t0\n",
            "Fake_516\t0\n",
            "Fake_517\t0\n",
            "Fake_518\t0\n",
            "Fake_519\t0\n",
            "Fake_521\t0\n",
            "Fake_531\t0\n",
            "Fake_541\t0\n",
            "Fake_551\t0\n",
            "Fake_561\t0\n",
            "Fake_571\t0\n",
            "Fake_581\t0\n",
            "Fake_591\t0\n",
            "Fake_601\t0\n",
            "Fake_610\t0\n",
            "Fake_611\t0\n",
            "Fake_612\t0\n",
            "Fake_613\t0\n",
            "Fake_614\t0\n",
            "Fake_615\t0\n",
            "Fake_616\t0\n",
            "Fake_617\t0\n",
            "Fake_618\t0\n",
            "Fake_619\t0\n",
            "Fake_621\t0\n",
            "Fake_631\t0\n",
            "Fake_641\t0\n",
            "Fake_651\t0\n",
            "Fake_661\t0\n",
            "Fake_671\t0\n",
            "Fake_681\t0\n",
            "Fake_691\t0\n",
            "Fake_701\t0\n",
            "Fake_710\t0\n",
            "Fake_711\t0\n",
            "Fake_712\t0\n",
            "Fake_713\t0\n",
            "Fake_714\t0\n",
            "Fake_715\t0\n",
            "Fake_716\t0\n",
            "Fake_717\t0\n",
            "Fake_718\t0\n",
            "Fake_719\t0\n",
            "Fake_721\t0\n",
            "Fake_731\t0\n",
            "Fake_741\t0\n",
            "Fake_751\t0\n",
            "Fake_761\t0\n",
            "Fake_771\t0\n",
            "Fake_781\t0\n",
            "Fake_791\t0\n",
            "Fake_801\t0\n",
            "Fake_810\t0\n",
            "Fake_811\t0\n",
            "Fake_812\t0\n",
            "Fake_813\t0\n",
            "Fake_814\t0\n",
            "Fake_815\t0\n",
            "Fake_816\t0\n",
            "Fake_817\t0\n",
            "Fake_818\t0\n",
            "Fake_819\t0\n",
            "Fake_821\t0\n",
            "Fake_831\t0\n",
            "Fake_841\t0\n",
            "Fake_851\t0\n",
            "Fake_861\t0\n",
            "Fake_871\t0\n",
            "Fake_881\t0\n",
            "Fake_891\t0\n",
            "Fake_901\t0\n",
            "Fake_910\t0\n",
            "Fake_911\t0\n",
            "Fake_912\t0\n",
            "Fake_913\t0\n",
            "Fake_914\t0\n",
            "Fake_915\t0\n",
            "Fake_916\t0\n",
            "Fake_917\t0\n",
            "Fake_918\t0\n",
            "Fake_919\t0\n",
            "Fake_921\t0\n",
            "Fake_931\t0\n",
            "Fake_941\t0\n",
            "Fake_951\t0\n",
            "Fake_961\t0\n",
            "Fake_971\t0\n",
            "Fake_981\t0\n",
            "Fake_991\t0\n",
            "Fake_1000\t0\n",
            "Fake_1001\t0\n",
            "Fake_1002\t0\n",
            "Fake_1003\t0\n",
            "Fake_1004\t0\n",
            "Fake_1005\t0\n",
            "Fake_1006\t0\n",
            "Fake_1007\t0\n",
            "Fake_1008\t0\n",
            "Fake_1009\t0\n",
            "Fake_1010\t0\n",
            "Fake_1011\t0\n",
            "Fake_1012\t0\n",
            "Fake_1013\t0\n",
            "Fake_1014\t0\n",
            "Fake_1015\t0\n",
            "Fake_1016\t0\n",
            "Fake_1017\t0\n",
            "Fake_1018\t0\n",
            "Fake_1019\t0\n"
          ]
        }
      ]
    }
  ]
}